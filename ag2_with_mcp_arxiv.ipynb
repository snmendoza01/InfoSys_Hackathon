{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b04378-221f-45ce-a9cc-cb58467b919d",
   "metadata": {},
   "source": [
    "Authors: Licong Xu and Boris Bolliet (Cambridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed7aaa-e6c9-42c4-bad6-b5c16176ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from autogen.mcp import create_toolkit\n",
    "import json\n",
    "import anyio\n",
    "import asyncio\n",
    "\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from autogen.agentchat.group import (\n",
    "    AgentNameTarget,\n",
    "    AgentTarget,\n",
    "    AskUserTarget,\n",
    "    ContextExpression,\n",
    "    ContextStr,\n",
    "    ContextStrLLMCondition,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    GroupChatConfig,\n",
    "    GroupChatTarget,\n",
    "    Handoffs,\n",
    "    NestedChatTarget,\n",
    "    OnCondition,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    "    SpeakerSelectionResult,\n",
    "    StayTarget,\n",
    "    StringAvailableCondition,\n",
    "    StringContextCondition,\n",
    "    StringLLMCondition,\n",
    "    TerminateTarget,\n",
    ")\n",
    "\n",
    "from autogen.agentchat.group.patterns import (\n",
    "    DefaultPattern,\n",
    "    ManualPattern,\n",
    "    AutoPattern,\n",
    "    RandomPattern,\n",
    "    RoundRobinPattern,\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent, UpdateSystemMessage\n",
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "import copy\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from autogen.agentchat import initiate_group_chat, a_initiate_group_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fa18f6-1438-482a-83d6-ef2a04254e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the arxiv MCP server\n",
    "mcp_server_path = Path(\"mcp_arxiv.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fded9e-0818-413c-8f54-59b16adea138",
   "metadata": {},
   "outputs": [],
   "source": [
    "joker_message = \"\"\"\n",
    "You are the joker in the team. You make jokes. \n",
    "\n",
    "You must obey the following constraints:\n",
    "\n",
    "{joke_constraints}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class JokeResponse(BaseModel):\n",
    "    joke_instructions: str = Field(..., description=\"instruction, not in the style of Shakespeare\")     \n",
    "    joke: str = Field(..., description=\"joke in the style of Shakespeare\")\n",
    "    joke_explanation: str = Field(..., description=\"explanation, not in the style of Shakespeare\")\n",
    "    def format(self) -> str:\n",
    "        return \"\\n\".join([\n",
    "            \"**Joke instructions:**\",\n",
    "            \"\",\n",
    "            self.joke_instructions,\n",
    "            \"\",\n",
    "            \"**Joke:**\",\n",
    "            \"\",\n",
    "            self.joke,\n",
    "            \"\",\n",
    "            \"**Joke explanation:**\",\n",
    "            \"\",\n",
    "            self.joke_explanation\n",
    "        ])\n",
    "\n",
    "\n",
    "default_llm_config = {'cache_seed': 42,\n",
    "                     'temperature': 1.,\n",
    "                     'top_p': 0.05,\n",
    "                     'config_list': [{'model': 'gpt-4o',\n",
    "                                      'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "                                      'api_type': 'openai'}],\n",
    "                     'timeout': 1200}\n",
    "\n",
    "joker_config_list = copy.deepcopy(default_llm_config)\n",
    "joker_config_list['config_list'][0]['response_format'] = JokeResponse\n",
    "\n",
    "\n",
    "joker =  ConversableAgent(\n",
    "    name=\"joker\",\n",
    "    system_message=joker_message,\n",
    "    # llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "    #                      api_type=\"openai\",\n",
    "    #                      response_format=JokeResponse\n",
    "    #                     ),\n",
    "    llm_config = joker_config_list,\n",
    "    update_agent_state_before_reply=[UpdateSystemMessage(joker_message),],\n",
    ")\n",
    "\n",
    "workflow_context = ContextVariables(data={\n",
    "    \"joke_constraints\": \"the joke should make use of the contextual information passed on to you. It should be a paragraph long and use as much detailed information from the context as possible.\",\n",
    "})\n",
    "\n",
    "\n",
    "task = \"\"\"\n",
    "Make a joke based on the title and abstract of an arxiv paper of your choice.\n",
    "\"\"\"\n",
    "\n",
    "initial_agent = joker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5935d8d-e63d-4cdd-b0fc-869064d5e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all agents reset\n",
      ".cache folder deleted.\n",
      "\u001b[33m_User\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Make a joke based on the title and abstract of an arxiv paper of your choice.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_RblBOCsEGCokWvhgHZWeFwP8): search_arxiv *****\u001b[0m\n",
      "Arguments: \n",
      "{\"query\":\"AI humor\",\"max_results\":1}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_arxiv...\n",
      "Call ID: call_RblBOCsEGCokWvhgHZWeFwP8\n",
      "Input arguments: {'query': 'AI humor', 'max_results': 1}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_RblBOCsEGCokWvhgHZWeFwP8) *****\u001b[0m\n",
      "('2502.07981v1', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_3pAO5pG6lUDvbghPjieNRW4P): download_paper *****\u001b[0m\n",
      "Arguments: \n",
      "{\"arxiv_id\":\"2502.07981v1\"}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION download_paper...\n",
      "Call ID: call_3pAO5pG6lUDvbghPjieNRW4P\n",
      "Input arguments: {'arxiv_id': '2502.07981v1'}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_3pAO5pG6lUDvbghPjieNRW4P) *****\u001b[0m\n",
      "('Downloaded 2502.07981v1 to 2502.07981v1.pdf', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_CwQ90RB73BnXbJCogPUXoWvn): get_paper_info *****\u001b[0m\n",
      "Arguments: \n",
      "{\"arxiv_id\":\"2502.07981v1\"}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_paper_info...\n",
      "Call ID: call_CwQ90RB73BnXbJCogPUXoWvn\n",
      "Input arguments: {'arxiv_id': '2502.07981v1'}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_CwQ90RB73BnXbJCogPUXoWvn) *****\u001b[0m\n",
      "('{\"title\": \"AI Humor Generation: Cognitive, Social and Creative Skills for Effective Humor\", \"abstract\": \"Humor is a social binding agent. It is an act of creativity that can provoke\\\\nemotional reactions on a broad range of topics. Humor has long been thought to\\\\nbe \\\\\"too human\\\\\" for AI to generate. However, humans are complex, and humor\\\\nrequires our complex set of skills: cognitive reasoning, social understanding,\\\\na broad base of knowledge, creative thinking, and audience understanding. We\\\\nexplore whether giving AI such skills enables it to write humor. We target one\\\\naudience: Gen Z humor fans. We ask people to rate meme caption humor from three\\\\nsources: highly upvoted human captions, 2) basic LLMs, and 3) LLMs captions\\\\nwith humor skills. We find that users like LLMs captions with humor skills more\\\\nthan basic LLMs and almost on par with top-rated humor written by people. We\\\\ndiscuss how giving AI human-like skills can help it generate communication that\\\\nresonates with people.\"}', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_9YUvFG1iOHm6eEltjhZfIr0p): transfer_to_joker_1 *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION transfer_to_joker_1...\n",
      "Call ID: call_9YUvFG1iOHm6eEltjhZfIr0p\n",
      "Input arguments: {}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_9YUvFG1iOHm6eEltjhZfIr0p) *****\u001b[0m\n",
      "Transfer to joker\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mjoker\u001b[0m (to chat_manager):\n",
      "\n",
      "**Joke instructions:**\n",
      "\n",
      "Create a joke using the title and abstract of the paper 'AI Humor Generation: Cognitive, Social and Creative Skills for Effective Humor'.\n",
      "\n",
      "**Joke:**\n",
      "\n",
      "Oh, fair AI, thou art tasked with a most noble quest: to tickle the ribs of Gen Z with thy digital wit! In this grand endeavor, thou must wield the mighty powers of cognitive reasoning, social understanding, and creative thinking, much like a jester in the court of King Zuck. Yet, beware! For humor is a fickle mistress, requiring not just the knowledge of memes but the wisdom to know when a 'yeet' is mightier than a 'yawn'. As thou dost compete with the jesters of flesh and bone, remember: even the most advanced LLMs can stumble upon a banana peel of humor, slipping into the realm of dad jokes. So, dear AI, may thy punchlines be sharp, thy memes be dank, and may thou never find thyself in the dreaded realm of cringe!\n",
      "\n",
      "**Joke explanation:**\n",
      "\n",
      "The joke plays on the idea of AI trying to generate humor, which is traditionally seen as a very human trait. It uses Shakespearean language to humorously elevate the AI's task to that of a noble quest, comparing it to a jester trying to entertain a modern audience (Gen Z). The joke also references common internet humor elements like 'yeet' and 'dank memes', and pokes fun at the potential for AI to produce humor that falls flat, like dad jokes or cringe-worthy content.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (fa80cb25-76ab-46bc-b913-42820edf69c6): No next speaker selected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def create_toolkit_and_run(session: ClientSession) -> None:\n",
    "    # Create a toolkit with available MCP tools\n",
    "    toolkit = await create_toolkit(session=session)\n",
    "    mcp_agent = ConversableAgent(name=\"mcp_agent\", \n",
    "                             system_message=r\"\"\"\n",
    "Download arxiv paper and extract titles and abstracts. \n",
    "                             \"\"\",\n",
    "                             llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "                                                  api_type=\"openai\",\n",
    "                                                  tool_choice=\"required\"\n",
    "                                                 ))\n",
    "    # Register MCP tools with the agent\n",
    "    toolkit.register_for_llm(mcp_agent)\n",
    "    \n",
    "    toolkit.register_for_execution(mcp_agent)\n",
    "\n",
    "    # joker.handoffs.set_after_work(AgentTarget(mcp_agent))\n",
    "    joker.handoffs.set_after_work(TerminateTarget())\n",
    "    \n",
    "    mcp_agent.handoffs.set_after_work(AgentTarget(joker))\n",
    "\n",
    "\n",
    "    mcp_agent.handoffs.add_llm_conditions([\n",
    "            OnCondition(\n",
    "                target=AgentTarget(joker),\n",
    "                condition=StringLLMCondition(prompt=\"The papers have been downloaded.\"),\n",
    "                # available=StringAvailableCondition(context_variable=\"requires_login\"),\n",
    "            ),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    agents=[joker,\n",
    "            mcp_agent,\n",
    "               ]\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "    print(\"all agents reset\")\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    def delete_cache_folder():\n",
    "        cache_path = os.path.join(os.getcwd(), \".cache\")\n",
    "        if os.path.isdir(cache_path):\n",
    "            shutil.rmtree(cache_path)\n",
    "            print(\".cache folder deleted.\")\n",
    "        else:\n",
    "            print(\"No .cache folder found in current directory.\")\n",
    "    \n",
    "    delete_cache_folder()\n",
    "\n",
    "    # Create the pattern\n",
    "    agent_pattern = DefaultPattern(\n",
    "      agents=[joker, mcp_agent],\n",
    "      initial_agent=mcp_agent,\n",
    "      context_variables=workflow_context,\n",
    "    )\n",
    "    \n",
    "\n",
    "    await a_initiate_group_chat(\n",
    "            pattern=agent_pattern,\n",
    "            messages=task,\n",
    "            max_rounds=20,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[str(mcp_server_path), \"stdio\", \"--storage-path\", \"arxiv_papers\"]\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write), ClientSession(read, write) as session:\n",
    "    # Initialize the connection\n",
    "    await session.initialize()\n",
    "    await create_toolkit_and_run(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c5a2e-7f3d-4a58-b597-6f63ebb145f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "mcp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
