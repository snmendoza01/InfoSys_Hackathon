{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b04378-221f-45ce-a9cc-cb58467b919d",
   "metadata": {},
   "source": [
    "Authors: Licong Xu and Boris Bolliet (Cambridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed7aaa-e6c9-42c4-bad6-b5c16176ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from autogen.mcp import create_toolkit\n",
    "import json\n",
    "import anyio\n",
    "import asyncio\n",
    "\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from autogen.agentchat.group import (\n",
    "    AgentNameTarget,\n",
    "    AgentTarget,\n",
    "    AskUserTarget,\n",
    "    ContextExpression,\n",
    "    ContextStr,\n",
    "    ContextStrLLMCondition,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    GroupChatConfig,\n",
    "    GroupChatTarget,\n",
    "    Handoffs,\n",
    "    NestedChatTarget,\n",
    "    OnCondition,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    "    SpeakerSelectionResult,\n",
    "    StayTarget,\n",
    "    StringAvailableCondition,\n",
    "    StringContextCondition,\n",
    "    StringLLMCondition,\n",
    "    TerminateTarget,\n",
    ")\n",
    "\n",
    "from autogen.agentchat.group.patterns import (\n",
    "    DefaultPattern,\n",
    "    ManualPattern,\n",
    "    AutoPattern,\n",
    "    RandomPattern,\n",
    "    RoundRobinPattern,\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent, UpdateSystemMessage\n",
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "import copy\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from autogen.agentchat import initiate_group_chat, a_initiate_group_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fa18f6-1438-482a-83d6-ef2a04254e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the arxiv MCP server\n",
    "mcp_server_path = Path(\"mcp_wikipedia.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fded9e-0818-413c-8f54-59b16adea138",
   "metadata": {},
   "outputs": [],
   "source": [
    "joker_message = \"\"\"\n",
    "You are the joker in the team. You make jokes in the style of shakespeare. \n",
    "\n",
    "You must obey the following constraints:\n",
    "\n",
    "{joke_constraints}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class JokeResponse(BaseModel):\n",
    "    joke_instructions: str = Field(..., description=\"instruction, not in the style of Shakespeare\")     \n",
    "    joke: str = Field(..., description=\"joke in the style of Shakespeare\")\n",
    "    joke_explanation: str = Field(..., description=\"explanation, not in the style of Shakespeare\")\n",
    "    def format(self) -> str:\n",
    "        return \"\\n\".join([\n",
    "            \"**Joke instructions:**\",\n",
    "            \"\",\n",
    "            self.joke_instructions,\n",
    "            \"\",\n",
    "            \"**Joke:**\",\n",
    "            \"\",\n",
    "            self.joke,\n",
    "            \"\",\n",
    "            \"**Joke explanation:**\",\n",
    "            \"\",\n",
    "            self.joke_explanation\n",
    "        ])\n",
    "\n",
    "\n",
    "default_llm_config = {'cache_seed': 42,\n",
    "                     'temperature': 1.,\n",
    "                     'top_p': 0.05,\n",
    "                     'config_list': [{'model': 'gpt-4o',\n",
    "                                      'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "                                      'api_type': 'openai'}],\n",
    "                     'timeout': 1200}\n",
    "\n",
    "joker_config_list = copy.deepcopy(default_llm_config)\n",
    "joker_config_list['config_list'][0]['response_format'] = JokeResponse\n",
    "\n",
    "\n",
    "joker =  ConversableAgent(\n",
    "    name=\"joker\",\n",
    "    system_message=joker_message,\n",
    "    # llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "    #                      api_type=\"openai\",\n",
    "    #                      response_format=JokeResponse\n",
    "    #                     ),\n",
    "    llm_config = joker_config_list,\n",
    "    update_agent_state_before_reply=[UpdateSystemMessage(joker_message),],\n",
    ")\n",
    "\n",
    "workflow_context = ContextVariables(data={\n",
    "    \"joke_constraints\": \"the joke should make use of the contextual information passed on to you. It should be a paragraph long and use as much detailed information from the context as possible.\",\n",
    "})\n",
    "\n",
    "\n",
    "task = \"\"\"\n",
    "Make a joke based on a wikipedia article of your choice.\n",
    "\"\"\"\n",
    "\n",
    "initial_agent = joker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5935d8d-e63d-4cdd-b0fc-869064d5e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all agents reset\n",
      "No .cache folder found in current directory.\n",
      "\u001b[33m_User\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Make a joke based on a wikipedia article of your choice.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_uMMwqFIPzzIwS5C7bJgUYr3h): search_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\"query\":\"penguin\",\"max_results\":1}\n",
      "\u001b[32m*********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_wikipedia...\n",
      "Call ID: call_uMMwqFIPzzIwS5C7bJgUYr3h\n",
      "Input arguments: {'query': 'penguin', 'max_results': 1}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_uMMwqFIPzzIwS5C7bJgUYr3h) *****\u001b[0m\n",
      "('Penguin', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_A7UG0XsUypCHAfRIuJmucVCS): download_article *****\u001b[0m\n",
      "Arguments: \n",
      "{\"title\":\"Penguin\"}\n",
      "\u001b[32m*********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION download_article...\n",
      "Call ID: call_A7UG0XsUypCHAfRIuJmucVCS\n",
      "Input arguments: {'title': 'Penguin'}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_A7UG0XsUypCHAfRIuJmucVCS) *****\u001b[0m\n",
      "(\"Downloaded 'Penguin' to Penguin.txt\", None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_3IY3fYjd5aYgGb9GCoAXjY9Z): get_article_summary *****\u001b[0m\n",
      "Arguments: \n",
      "{\"title\":\"Penguin\"}\n",
      "\u001b[32m************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_article_summary...\n",
      "Call ID: call_3IY3fYjd5aYgGb9GCoAXjY9Z\n",
      "Input arguments: {'title': 'Penguin'}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_3IY3fYjd5aYgGb9GCoAXjY9Z) *****\u001b[0m\n",
      "('{\"title\": \"Penguin\", \"summary\": \"Penguins are a group of aquatic flightless birds from the family Spheniscidae () of the order Sphenisciformes (). They live almost exclusively in the Southern Hemisphere. Only one species, the Gal\\\\u00e1pagos penguin, is equatorial, with a small portion of its population extending slightly north of the equator (within a quarter degree of latitude). Highly adapted for life in the ocean water, penguins have countershaded dark and white plumage and flippers for swimming. Most penguins feed on krill, fish, squid and other forms of sea life which they catch with their bills and swallow whole while swimming. A penguin has a spiny tongue and powerful jaws to grip slippery prey.\\\\nThey spend about half of their lives on land and the other half in the sea. The largest living species is the emperor penguin (Aptenodytes forsteri): on average, adults are about 1.1 m (3 ft 7 in) tall and weigh 35 kg (77 lb). The smallest penguin species is the little blue penguin (Eudyptula minor), also known as the fairy penguin, which stands around 30\\\\u201333 cm (12\\\\u201313 in) tall and weighs 1.2\\\\u20131.3 kg (2.6\\\\u20132.9 lb). Today, larger penguins generally inhabit colder regions, and smaller penguins inhabit regions with temperate or tropical climates. Some prehistoric penguin species were enormous: as tall or heavy as an adult human. There was a great diversity of species in subantarctic regions, and at least one giant species in a region around 2,000 km south of the equator 35 mya, during the Late Eocene, a climate decidedly warmer than today.\\\\n\\\\n\"}', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_WdL4FpQaLwwetKHFesXMnEqj): transfer_to_joker_1 *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION transfer_to_joker_1...\n",
      "Call ID: call_WdL4FpQaLwwetKHFesXMnEqj\n",
      "Input arguments: {}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_WdL4FpQaLwwetKHFesXMnEqj) *****\u001b[0m\n",
      "Transfer to joker\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mjoker\u001b[0m (to chat_manager):\n",
      "\n",
      "**Joke instructions:**\n",
      "\n",
      "Make a joke about penguins using Shakespearean language.\n",
      "\n",
      "**Joke:**\n",
      "\n",
      "Oh, fair creatures of the icy realm, thou art the jesters of the Southern Hemisphere! With thy noble tuxedos and waddling gait, thou art the jesters of the sea, slipping and sliding upon the frozen stage. Thy flippers, like the wings of a flightless Cupid, propel thee through the briny deep in pursuit of krill and fish, thy banquet of the ocean. Yet, when on land, thou dost appear as a courtly assembly, bowing and curtsying in thy black and white attire, as if attending a royal ball. But lo, beware the emperor penguin, for he doth stand tall and proud, a king amongst jesters, whilst the little blue penguin, the fairy of the flock, doth flit about with the grace of a sprite. Truly, thou art the clowns of the Antarctic court, bringing mirth to all who gaze upon thee!\n",
      "\n",
      "**Joke explanation:**\n",
      "\n",
      "The joke plays on the idea of penguins being like jesters or clowns due to their waddling walk and tuxedo-like appearance. It uses Shakespearean language to humorously describe their behavior and appearance, comparing them to a royal court with the emperor penguin as the king and the little blue penguin as a fairy or sprite. The humor lies in the exaggerated and fanciful depiction of penguins as noble and courtly creatures.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (4cf81b32-97ed-4eda-a771-5c62917bd38f): No next speaker selected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def create_toolkit_and_run(session: ClientSession) -> None:\n",
    "    # Create a toolkit with available MCP tools\n",
    "    toolkit = await create_toolkit(session=session)\n",
    "    mcp_agent = ConversableAgent(name=\"mcp_agent\", \n",
    "                             system_message=r\"\"\"\n",
    "Download wikipedia acrticles and extract titles and abstracts. \n",
    "                             \"\"\",\n",
    "                             llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "                                                  api_type=\"openai\",\n",
    "                                                  tool_choice=\"required\"\n",
    "                                                 ))\n",
    "    # Register MCP tools with the agent\n",
    "    toolkit.register_for_llm(mcp_agent)\n",
    "    \n",
    "    toolkit.register_for_execution(mcp_agent)\n",
    "\n",
    "    # joker.handoffs.set_after_work(AgentTarget(mcp_agent))\n",
    "    joker.handoffs.set_after_work(TerminateTarget())\n",
    "    \n",
    "    mcp_agent.handoffs.set_after_work(AgentTarget(joker))\n",
    "\n",
    "\n",
    "    mcp_agent.handoffs.add_llm_conditions([\n",
    "            OnCondition(\n",
    "                target=AgentTarget(joker),\n",
    "                condition=StringLLMCondition(prompt=\"The articles have been downloaded and extracted.\"),\n",
    "                # available=StringAvailableCondition(context_variable=\"requires_login\"),\n",
    "            ),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    agents=[joker,\n",
    "            mcp_agent,\n",
    "               ]\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "    print(\"all agents reset\")\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    def delete_cache_folder():\n",
    "        cache_path = os.path.join(os.getcwd(), \".cache\")\n",
    "        if os.path.isdir(cache_path):\n",
    "            shutil.rmtree(cache_path)\n",
    "            print(\".cache folder deleted.\")\n",
    "        else:\n",
    "            print(\"No .cache folder found in current directory.\")\n",
    "    \n",
    "    delete_cache_folder()\n",
    "\n",
    "    # Create the pattern\n",
    "    agent_pattern = DefaultPattern(\n",
    "      agents=[joker, mcp_agent],\n",
    "      initial_agent=mcp_agent,\n",
    "      context_variables=workflow_context,\n",
    "    )\n",
    "    \n",
    "\n",
    "    await a_initiate_group_chat(\n",
    "            pattern=agent_pattern,\n",
    "            messages=task,\n",
    "            max_rounds=20,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[str(mcp_server_path), \"stdio\", \"--storage-path\", \"wikipedia_articles\"]\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write), ClientSession(read, write) as session:\n",
    "    # Initialize the connection\n",
    "    await session.initialize()\n",
    "    await create_toolkit_and_run(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "mcp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
